[
  {
    "title": "VizXpress: ",
    "authors": ["Lotus Zhang", "Zhuohao (Jerry) Zhang", "Gina Clepper", "Franklin Mingzhe Li", "Patrick Carrington", "Jacob O. Wobbrock", "Leah Findlater"],
    "conference": "ASSETS",
    "year": 2025,
    "image": "",
    "alt": "",
    "link": "",
    "abstract": "",
    "feature": true
  },{
    "title": "Modeling Accessibility: Characterizing What We Mean By “Accessible”",
    "authors": ["Kelly Avery Mack", "Jesse J. Martinez", "Aaleyah Lewis", "Jennifer Mankoff", "James Fogarty", "Leah Findlater", "Heather D. Evans", "Cynthia L. Bennett", "Emma J. McDonnell"],
    "conference": "ASSETS",
    "year": 2025,
    "image": "",
    "alt": "",
    "link": "",
    "abstract": "",
    "feature": true
  },{
    "title": "“What Would I Want to Make? Probably Everything”: Practices and Speculations of Blind and Low Vision Tactile Graphics Creators",
    "authors": ["Gina Clepper", "Emma J. McDonnell", "Leah Findlater", "Nadya Peek"],
    "conference": "CHI",
    "year": 2025,
    "image": "clepperchi2025.jpg",
    "alt": "",
    "link": "https://doi.org/10.1145/3706598.3714173",
    "abstract": "Tactile graphics communicate images and spatial information to blind and low vision (BLV) audiences via touch. However, designing and producing tactile graphics is laborious and often inaccessible to BLV people themselves. We interviewed 14 BLV adults with experience both using and creating tactile graphics to understand their current and desired practices. We found that tactile graphics are intensely valued by many, but that access to and fluency with tactile graphics are compounding challenges. To produce tactile graphics, BLV makers constantly navigate tradeoffs between accessible, low-fidelity craft materials and less accessible, high-fidelity equipment. Going forward, we argue that tactile graphics design and production should be made widely accessible and that tactile graphics themselves should be designed to be expressive and ubiquitous. Drawing from these design goals, we propose specific future tools with features for inclusive designing, sharing, and (re)production of tactile graphics.",
    "feature": true
  },{
    "title": "SPECTRA: Personalizable Sound Recognition for Deaf and Hard of Hearing Users through Interactive Machine Learning",
    "authors": ["Steven Goodman", "Emma J. McDonnell", "Jon E. Froehlich", "Leah Findlater"],
    "conference": "CHI",
    "year": 2025,
    "image": "goodmanchi2025.jpg",
    "alt": "",
    "link": "https://doi.org/10.1145/3706598.3713294",
    "abstract": "We introduce SPECTRA, a novel pipeline for personalizable sound recognition designed to understand DHH users’ needs when collecting audio data, creating a training dataset, and reasoning about the quality of a model. To evaluate the prototype, we recruited 12 DHH participants who trained personalized models for their homes. We investigated waveforms, spectrograms, interactive clustering, and data annotating to support DHH users throughout this workflow, and we explored the impact of a hands-on training session on their experience and attitudes toward sound recognition tools. Our findings reveal the potential for clustering visualizations and waveforms to enrich users’ understanding of audio data and refinement of training datasets, along with data annotations to promote varied data collection. We provide insights into DHH users’ experiences and perspectives on personalizing a sound recognition pipeline. Finally, we share design considerations for future interactive systems to support this population.",
    "feature": true
  },{
    "title": "Making Urban Art Accessible: Current Art Access Techniques, Design Considerations, and the Role of AI",
    "authors": ["Lucy Jiang", "Jon E. Froehlich", "Leah Findlater"],
    "conference": "ASSETS Workshop (The Future of Urban Accessibility: The Role of AI)",
    "year": 2024,
    "image": "jiangassets2024.jpg",
    "alt": "Examples to show a range of public art types: murals, mosaic, sculpture, and graffiti art.",
    "link": "https://doi.org/10.48550/arXiv.2410.20571",
    "abstract": "Public artwork, from vibrant wall murals to captivating sculptures, can enhance the aesthetic of urban spaces, foster a sense of community and cultural identity, and help attract visitors. Despite its benefits, most public art is visual, making it often inaccessible to blind and low vision (BLV) people. In this workshop paper, we first draw on art literature to help define the space of public art, identify key differences with curated art shown in museums or galleries, and discuss implications for accessibility. We then enumerate how existing art accessibility techniques may (or may not) transfer to urban art spaces. We close by presenting future research directions and reflecting on the growing role of AI in making art accessible.",
    "feature": true
  },{
    "title": "Envisioning Collective Communication Access: A Theoretically-Grounded Review of Captioning Literature from 2013-2023",
    "authors": ["Emma J. McDonnell", "Leah Findlater"],
    "conference": "ASSETS",
    "year": 2024,
    "image": "mcdonnellassets2024.png",
    "alt": "Line graph showing the number of captioning papers published per year with an increasing trend from 2013 2023.",
    "link": "https://dl.acm.org/doi/10.1145/3663548.3675649",
    "abstract": "A significant body of human-computer interaction accessibility research explores ways technology can improve communication access. Yet, this research infrequently engages other fields with complementary expertise – namely disability studies, Deaf studies, disability justice, and communication studies. To facilitate interdisciplinary communication access research, we synthesize thinking from these four fields into a framework of collective communication access. We then analyze human-centered accessibility-focused captioning research published between 2013 and 2023, investigating how collective communication access principles are or are not employed. We find that, while the majority of captioning research does not demonstrate a collective communication access approach, it reaches a baseline of targeting change toward inaccessible technical infrastructures and engaging d/Deaf and hard of hearing people as captioning experts. The small body of work that aligns with our framework, however, demonstrates that designing to change discriminatory social conditions and engaging conversation partners in access is a promising direction for future work.",
    "feature": true
  },{
    "title": "“Caption It in an Accessible Way That Is Also Enjoyable”: Characterizing User-Driven Captioning Practices on TikTok",
    "authors": ["Emma J. McDonnell", "Tessa Eagle", "Pitch Sinlapanuntakul", "Soo Hyun Moon", "Kathryn Ringland", "Jon E. Froehlich", "Leah Findlater"],
    "conference": "CHI",
    "year": 2024,
    "image": "mcdonnellchi2024.png",
    "alt": "Simulated screenshot of a TikTok showing closed captions at the bottom of the video and stylized open captions on the top half.",
    "link": "https://ej-mcdonnell.github.io/Caption%20It%20In%20An%20Accessible%20Way%20That%20is%20Also%20Enjoyable%20--%20accessible%20PDF.pdf",
    "abstract": "As user-generated video dominates media landscapes, it poses an accessibility challenge. While disability advocacy groups globally have secured hard-won accessibility regulations for broadcast media, no such regulation of user-generated content exists. Yet, one major player in this shift, TikTok, has a culture of user-generated, creative captioning. We sought to understand how TikTok videos are captioned and the impact current practices have on those who need captions to access audio content. Therefore, we conducted a content analysis of 300 open-captioned TikToks and contextualized these findings by interviewing nine caption users. We found that the current state of TikTok captioning does facilitate access to the platform but that a user-generated, social video-specific standard for captioning could improve caption quality and expand access. We contribute an empirical account of the state of TikTok captioning and outline steps toward a standard for user-generated captioning.",
    "feature": true
  },{
    "title": "Designing Accessible Obfuscation Support for Blind Individuals’ Visual Privacy Management",
    "authors": ["Lotus Zhang", "Abigale Stangl", "Tanusree Sharma", "Yu-Yun Tseng", "Inan Xu", "Danna Gurari", "Yang Wang", "Leah Findlater"],
    "conference": "CHI",
    "year": 2024,
    "image": "zhangchi2024.png",
    "alt": "Sequence of screeshots from experimental app showing a user editing an image of mangoes.",
    "link": "https://dl.acm.org/doi/10.1145/3613904.3642713",
    "abstract": "Blind individuals commonly share photos in everyday life. Despite substantial interest from the blind community in being able to independently obfuscate private information in photos, existing tools are designed without their inputs. In this study, we prototyped a preliminary screen reader-accessible obfuscation interface to probe for feedback and design insights. We implemented a version of the prototype through off-the-shelf AI models (e.g., SAM, BLIP2, ChatGPT) and a Wizard-of-Oz version that provides human-authored guidance. Through a user study with 12 blind participants who obfuscated diverse private photos using the prototype, we uncovered how they understood and approached visual private content manipulation, how they reacted to frictions such as inaccuracy with existing AI models and cognitive load, and how they envisioned such tools to be better designed to support their needs (e.g., guidelines for describing visual obfuscation effects, co-creative interaction design that respects blind users’ agency).",
    "feature": true
  },{
    "title": "“Easier or Harder, Depending on Who the Hearing Person Is”: Codesigning Videoconferencing Tools for Small Groups with Mixed Hearing Status",
    "authors": ["Emma J. McDonnell", "Soo Hyun Moon", "Lucy Jiang", "Steven M. Goodman", "Raja Kushalnagar", "Jon E. Froehlich", "Leah Findlater"],
    "conference": "CHI",
    "year": 2023,
    "image": "mcdonnellchi2023.png",
    "alt": "Three people in a zoom conversation with a warning that multiple people are speaking",
    "link": "https://ej-mcdonnell.github.io/Easier%20or%20Harder%20Depending%20on%20Who%20the%20Hearing%20Person%20Is%20-%20Accessible%20Final%20PDF.pdf",
    "abstract":"With improvements in automated speech recognition and increased use of videoconferencing, real-time captioning has changed significantly. This shift toward broadly available but less accurate captioning invites exploration of the role hearing conversation partners play in shaping the accessibility of a conversation to d/Deaf and hard of hearing (DHH) captioning users. While recent work has explored DHH individuals’ videoconferencing experiences with captioning, we focus on established groups’ current practices and priorities for future tools to support more accessible online conversations. Our study consists of three codesign sessions, conducted with four groups (17 participants total, 10 DHH, 7 hearing). We found that established groups crafted social accessibility norms that met their relational contexts. We also identify promising directions for future captioning design, including the need to standardize speaker identification and customization, opportunities to provide behavioral feedback during a conversation, and ways that videoconferencing platforms could enable groups to set and share norms.",
    "feature": true
  }
]
